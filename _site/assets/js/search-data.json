{
  
  "0": {
    "title": "Getting started",
    "content": "Getting started with cgpipe .",
    "url": "http://0.0.0.0:4000/cgpipe/docs/getting-started",
    "relUrl": "/docs/getting-started"
  }
  ,"1": {
    "title": "What is cgpipe?",
    "content": "What is cgpipe? . Quick version . CGPipe is a language for building data analysis pipelines. It is a declarative programming language similar to Makefiles, however instead of directly executing target scripts, jobs are submitted to a dedicated job scheduler, such as PBS, SGE, or SLURM. . You define output files, which input files they need, and the script required to get from input &gt; output. Each output file (or job) is defined separately. Taken together, these job definitions form a directed acyclic graph (DAG). . input &gt; file1 --&gt; file2 &gt; file3 &gt; ... &gt; output1 -&gt; file1b &gt; output2 . or map-reduce style… . # split input input &gt; file1 + file2 + file3 # map file1 &gt; fileA file2 &gt; fileB file3 &gt; fileC #reduce fileA + fileB + fileC &gt; output . You give CGPipe these definitions and the name of the file you ultimately want. Knowing these task definitions and the desired output, CGPipe will figure out how to &quot;walk the graph&quot; to figure out what files/jobs are needed. It also backtracks the walk in case there are multiple ways to get to your output. CGPipe will look first on-disk for required input files. It will only submit a job to build a missing or out of date file. If an input is newer than an output, that output file is considered out of date. So far, this is the same as how make works, except for the job submission part… where make executes the tasks, CGPipe submits them as jobs to a scheduler. . Similarities and differences to Makefiles . CGPipe pipelines have a very simliar syntax to Makefiles, particularly in the way that build-targets are defined. . Jobs are defined based on the files they will produce as output. The pipeline author can define which output files will be created, and which input files are required for a particular task. After that, a shell script snippet is included that defines the commands that need to be executed to produce the defined output files. . | Jobs dependencies are automatically calculated and a build-graph produced. Jobs are then executed in the proper order along that build-graph to produce the required outputs. . | CGPipe pipelines can be included in other pipelines. . | If a given output file exists, it will not be rebuilt unless a defined input file will be rebuilt. CGPipe extends this to also track outputs/jobs that have been submitted to a job scheduler. If an built-target requires an input that has already been submitted to the scheduler, that input will not be resubmitted, rather the existing job will be listed as a job dependency. . | . In this way, CGPipe is very similar to the qmake program that is available for SGE clusters, which executes unmodified Makefiles using the SGE scheduler. This allows for some parallelization. But there are some key differences between CGPipe pipelines and using qmake to execute an unaltered Makefile. . CGPipe allows you to specify job requirements, such as execution time, CPUs, and memory. These requirements can be set on a job or pipeline basis, allowing the pipeline author to set requirements globally or for only individual tasks. For example, the account setting can be set for all tasks, but walltime could be set on a per-task basis. . | qmake runs interactively, submitting jobs in order and waiting for the results before submitting the next job. Because of this, qmake needs to keep running on the job submission host. With CGPipe, the entire pipeline is submitted to the job scheduler at once. There is no need to have a watchdog script running, as the job scheduler will automatically cancel any dependent jobs in the event of an error. . | Command line arguments can be easily used to set variables within the script. . | Makefiles don&#39;t include any type of flow control (if/else, for-loops), but CGPipe is a full language that includes if/else conditions and for-loops for iterating over a list of values or a range. Everything in a CGPipe line can be scripted, including target definitions. This means that an author could define a pipeline that executed in a Map-Reduce pattern where an input file is split into N number of chunks, each chunk could be processed in parallel, and then the results could be merged back together after each chunk was processed. This type of pipeline could be written in a traditional Makefile (verbosely), but by allowing build targets to be included in for-loops, the number of chunks can now be a run-time option and written in an easily readable syntax. . | Build targets are scriptable with CGPipe templates . | qmake is only available for SGE clusters and isn&#39;t available for other job schedulers. CGPipe pipelines can execute on SGE or SLURM systems as well as on single hosts with SBS or bash script exports (see below). . | Multiple targets can be defined for the same output files, enabling multiple execution paths to build the same output files. This means that there can be multiple set of jobs defined to yield the same output file(s), based upon what input files are available. For example, you could have paired-end next-generation sequencing reads stored in two separate FASTQ files or one interleaved FASTQ file. Either of these inputs could be used in a read alignment step to produce a BAM file, but the arguments for the alignment program may be slightly different, depending on which type of input is used. With CGPipe, you can specify two different targets for the same output file, with the targets prioritized in the order they are defined in the pipeline. If the first target (or any of its inputs) can&#39;t be used, then the next target is attempted until all possible build-graphs are exhausted. . | Pipelines can be stored remotely, such as on a GitHub repository or web server. . | Pipelines can be used as executable scripts using the #! (shebang) first line syntax (like bash, perl, ruby, or python scripts). . | Pipelines can display help text. If the first lines are comments, then they will be displayed when help text is requested (excluding #! first lines). The first blank or non-commented line marks the end of the help text. . | . Executing pipelines . It is expected that jobs will be executed on an HPC cluster with a job scheduler (SGE, SLURM, or PBS supported). This way individual tasks can be efficiently executed in a parallel manner. CGPipe will take care of setting inter-task dependencies to make sure that jobs execute in the proper order. Pipelines can also be run on a single host by using either the simple SBS scheduler or by exporting the pipeline as a bash script. SBS is well suited for single-host systems where there is no existing job scheduler. SBS requires having the sbs program installed somewhere in your $PATH. .",
    "url": "http://0.0.0.0:4000/cgpipe/docs/getting-started/what-is-cgpipe",
    "relUrl": "/docs/getting-started/what-is-cgpipe"
  }
  ,"2": {
    "title": "Quick version",
    "content": "The quick version… . CGpipe is langauge for defining and building data analysis pipelines that run on an HPC cluster (for bioinformatics). The cgpipe program helps to submit jobs to the cluster and keeps track of jobs. . With cgpipe, a long set of submission scripts can be reduced to a single command. But, as a language, the pipelines you write are adaptable and configurable. This means that your single command can now work for a variety of situations. . So, instead of having a separate RNAseq script that is hard-coded for each project, you can now have one cgpipe script that accepts arguments for: different FASTQ files for input, different genome indexes, and any extra filtering steps. This will make your data pipelines easier to manage and more reproducible across multiple projects. .",
    "url": "http://0.0.0.0:4000/cgpipe/docs/getting-started/quick",
    "relUrl": "/docs/getting-started/quick"
  }
  ,"3": {
    "title": "Installation",
    "content": "Installation . Note: CGPipe is intended to run on a Unix/Linux workstation, server, or cluster (including macOS). . CGPipe is a Java program that is packaged as either a self-executing fat JAR file, or as an embeddable library. CGPipe is a Java program, but it has been developed to run on *nix-style hosts such as Mac OSX, FreeBSD or Linux. The only pre-requisite is a working installation of Java 1.7 or better. It is untested on Windows. . Step 0: Install Java . Java version 7 or higher can be used. Install Java from your Linux distribution (using apt-get or yum), or see the Oracle site for more details. . Step 1: Download cgpipe . You can download the cgpipe program from the CGPipe website. You can save this file anywhere, but it is easiest to use if it is included on your $PATH somewhere, such as in /usr/local/bin or in a personal $HOME/bin directory. . Step 2: Run a test pipeline . Most analysis pipelines you run will be custom written, however you can verify that your CGPipe installation is working with the following script: . [hello.cgp] #!/usr/bin/env cgpipe print &quot;Hello from cgpipe!&quot; . This will load cgpipe from your path and execute the above script. Right now the script doesn&#39;t do anything other than print a message to the console. If you save this as hello.cgp (and make it executable with chmod +x hello.cgp), you should see the following: . $ ./hello.cgp Hello from cgpipe! . or… . $ cgpipe hello.cgp Hello from cgpipe! . We will build from here to demonstate how to make your own pipelines in the next sections. . Step 3: Configure CGPipe for your computing environment . CGPipe can run on a single user workstation, server, or HPC cluster. If you want to run more complex workflows by submitting jobs to a scheduler, it&#39;s necessary to configure CGPipe to use your scheduler. In CGPipe job submission is handled by &quot;job runners&quot;. . The currently supported job schedulers are: SBS, SGE, SLURM, or PBS. For more information about available runners, or the possible configuration settings, see &quot;Running jobs&quot;. . If no scheduler is configured, jobs will be written as a bash script to stdout. . For information about how to configure CGPipe, see the &quot;Configuring CGPipe&quot; section below. . Running CGPipe . As shown above, CGPipe can be run either from the command-line (cgpipe mypipeline.cgp) or a pipeline script can be made executable and cgpipe loaded with the shebang (!#) first line definition like any other scripting language. It is recommended that the script method be used and you install CGPipe to a location in your $PATH. This way, you can use the following format to start your script: . #!/usr/bin/env cgpipe . Configuring CGPipe . CGPipe configuration or job variables can be set on a per-run, per-user, or per-server basis. At startup, CGPipe looks for configuration information in the following locations (in order of preference): . From within the CGPipe JAR itself (path io/compgen/cgpipe/cgpiperc), | /etc/cgpiperc, | $CGPIPE_HOME/.cgpiperc (CGPIPE_HOME defaults to the directory containing the cgpipe binary), | ~/.cgpiperc (user-local config), | The environmental variable CGPIPE_ENV (semi-colon delimited) | These configuration files are CGPipe scripts that are &#39;included&#39; with your running CGPipe script. This means that they can inherit options from each other. . Examples: . Do you want to set job.mail for all of your personal scripts? Then set the value in ~/.cgpiperc. | Do you want to configure the batch scheduler settings for everyone on a given system? Then set job.runner in /etc/cgpiperc or $CGPIPE_HOME/.cgpiperc. | Do you have a network shared folder with CGPipe installed that is mounted across different clusters? Then you can still set the CGPipe runner config from $CGPIPE_HOME/.cgpiperc by setting the job.runner config values within an if/then/endif block (cgpiperc files are full cgpipe scripts that are interpretted, so you can make them quite customizeable). | .",
    "url": "http://0.0.0.0:4000/cgpipe/docs/getting-started/installing",
    "relUrl": "/docs/getting-started/installing"
  }
  ,"4": {
    "title": "Running cgpipe",
    "content": "Running cgpipe . . Using the program | Configuring cgpipe | Examples | Using the program . CGpipe can be run either from the command-line (cgpipe mypipeline.cgp). Or, it can be used in a pipeline script that starts with a shebang (!#) – just like any other scripting language! It is also recommended to copy cgpipe to a location in your $PATH. This way, you can use the following stub to start your script: . #!/usr/bin/env cgpipe print &quot;Hello pipeline!&quot; . Note: In most cases, you won&#39;t be executing cgpipe directly. Instead, you&#39;ll be writing pipeline scripts in cgpipe, and those are what you&#39;ll execute. . Configuring cgpipe . CGpipe configuration or job variables can be set on a per-run, per-user, or per-server basis. At startup, cgpipe looks for configuration information in the following locations (in order of preference): . From within the cgpipe JAR itself (path io/compgen/cgpipe/cgpiperc – this is only for default values), | /etc/cgpiperc (for server-level options), | $CGPIPE_HOME/.cgpiperc (CGPIPE_HOME defaults to the directory containing the cgpipe binary), | ~/.cgpiperc (user-local config), | The environmental variable CGPIPE_ENV (semi-colon delimited) | All of these configuration files are themselves cgpipe scripts that are &#39;included&#39; with your running cgpipe script. This means that they can inherit options from each other. . Examples . Do you want to set job.mail for all of your personal scripts? Then set the value in ~/.cgpiperc. | Do you want to configure the batch scheduler settings for everyone on a given system? Then set job.runner in /etc/cgpiperc. What is cgpipe is installed as an envirnmental module? Then use $CGPIPE_HOME/.cgpiperc. | . | Do you have cgpipe installed on a network drive that is mounted on different clusters? You can still set the cgpipe runner config from $CGPIPE_HOME/.cgpiperc by setting the job.runner config values. But, you can also use if/then/endif blocks to set system specifc values. The cgpiperc files are full scripts that are interpretted, so you can make them quite customizeable. | . Note: I&#39;ve actually had to do this. I had a network drive was mounted on two different clusters. One was CentOS 6 running PBS, the other was CentOS 7 running SGE. Jobs ran on both. .",
    "url": "http://0.0.0.0:4000/cgpipe/docs/getting-started/running",
    "relUrl": "/docs/getting-started/running"
  }
  ,"5": {
    "title": "Language syntax",
    "content": "Language syntax . The cgpipe language has a simple syntax that is similar to many other languages. The flow of a script is also similar to Makefiles. . The source code contains a set of test scripts that have examples of all statements and operations. These test scripts are the definitive source for the language syntax. These tests are run used to verify each build of cgpipe. In any case where this documentation conflicts with the test scripts, the test scripts are correct. . Test scripts are available in the src/test-scripts directory and are named *.cgpt or *.cgpipe. . . Contexts | Data types | Variables | Lists | Math | Logic | Variable substitution | Shell escaping | Printing | If/Else/Endif Conditions | | For loops | Build target definitions Wildcards in targets | Target substitutions | Special targets | | Including other files | Logging | Output logs | Comments | Help text | Job execution options Specifying the shell to use | Direct execution of jobs | | Experimental cgpipe language features Target snippets imports | Eval statement | Double evaluated variables | | Contexts . There are two contexts in a CGPipe pipeline: &quot;global&quot; and &quot;target&quot;. In the &quot;global&quot; context, all uncommented lines are evaluated and treated as CGPipe code. The &quot;target&quot; context is how job commands are defined. . Within a target context, the code is interpreted in &quot;template&quot; mode, where only areas wrapped in &lt;% %&gt; are evalutated as CGPipe code. The areas not wrapped in &lt;% %&gt; are treated as the body of the job script to execute. Any whitespace present in the target body is kept and not stripped. In a target, any print statements will be added to the target body, not written to the console. . When a target is defined, it captures the existing global context at definition (like a closure). During execution, target contexts are therefore disconnected from the global context. In practice, this means that a target can read a global variable (if it has been set prior to the build-target definition), however, a target can not set a global variable and have the new value be visible outside or it&#39;s own context. . A target is defined using the format: . output_file1 {output_file2 ... } : {input_file1 input_file2 ...} # indent to establish the target-context script body snippet &lt;% cgpipe-expression %&gt; script body snippet script body snippet &lt;% cgpipe-expression cgpipe-expression %&gt; ... # ends with outdent . Data types . There are 6 primary data types in CGPipe: boolean, float, integer, list, range and string. Booleans are either true or false (case-sensitive). Strings must be enclosed in double quotes. Lists are initialized using the syntax &quot;[]&quot;. Ranges can be used to iterate over a list of numbers using the syntax &quot;from..to&quot;. . Here are some examples: . foo = &quot;Hello world&quot; foo = 1 foo = 1.0 isvalid = true list = [] list += &quot;one&quot; list += &quot;two&quot; range = 1..10 . Variables . foo = &quot;val&quot; Set a variable . foo ?= &quot;val&quot; Set a variable if it hasn&#39;t already been set . foo += &quot;val&quot; Append a value to a list (if the variable has already been set, then this will convert that variable to a list) . unset foo Unsets a variable. Note: if the variable was used by a target, it will still be set within the context of the target. . Variables may also be set at the command-line like this: cgpipe -foo bar -baz 1 -baz 2. This is the same as saying: . foo = &quot;bar&quot; bar = 2.59 . Lists . You can also create and access elements in a list using the [] splice operator. List items don&#39;t have to be of the same data type, but it is recommended that they are. List indexing starts at zero. Negative indexes are treated as relative to the end of the list. . foo = [] foo = [1, 2, &quot;three&quot;] print foo[2] &gt;&gt;&gt; &quot;three&quot; print foo[-1] &gt;&gt;&gt; &quot;three&quot; . You can also append to lists: . foo = [&quot;foo&quot;] foo += &quot;bar&quot; foo += &quot;baz&quot; print foo &gt;&gt;&gt; &quot;foo bar baz&quot; . List elements can be sliced using the same [start:end] syntax as in Python. If start or end is omitted, it is assumed to be the 0 or len(list), respectively. . foo = [&quot;one&quot;, &quot;two&quot;, &quot;three&quot;] print foo[1:] &gt;&gt;&gt; two three print foo[:2] &gt;&gt;&gt; one two print foo[:-1] &gt;&gt;&gt; one two . Math . You can perform basic arithmetic on integer and float variables. Available operations are: . + add | - subtract | * multiplication | / divide (integer division if on an integer) | % remainder | ** power (2**3 = 8) | . Operations are performed in standard order; however, you can also add also parentheses around clauses to process things in a different order. For example: . 8 + 2 * 10 = 28 (8 + 2) * 10 = 100 8 + (2 * 10) = 28 . Logic . You can perform basic logic operations as well. This will most commonly be used in the context of an if-else condition. . &amp;&amp; and | || or | ! not (or is unset) | == equals | != not equals | &lt; less than | &lt;= less than or equals | &gt; greater than | &gt;= greater than or equals | . You can chain these together to form more complex conditions. For example: . foo = &quot;bar&quot; baz = 12 if foo == &quot;bar&quot; &amp;&amp; baz &lt; 20 print &quot;test&quot; endif . Variable substitution . Inside of strings, variables can be substituted. Each string (including build script snippets) will be evaluated for variable substitutions. . ${var} - Variable named &quot;var&quot;. If &quot;var&quot; is a list, ${var} will be replaced with a space-separated string with all members of the list. **If &quot;var&quot; hasn&#39;t been set, then this will throw a ParseError exception.** ${var?} - Optional variable substitution. This is the same as above, except that if &quot;var&quot; hasn&#39;t been set, then it will be replaced with an empty string: &#39;&#39;. foo_@{var}_bar - A replacement list, capturing the surrounding context. For each member of list, the following will be returned: foo_one_bar, foo_two_bar, foo_three_bar, etc... foo_@{n..m}_bar - A replacement range, capturing the surrounding context. For each member of range ({n} to {m}, the following will be returned: foo_1_bar, foo_2_bar, foo_3_bar, etc... {n} and {m} may be variables or integers . Shell escaping . You may also include the results from shell commands as well using the syntax $(command). Anything surrounded by $() will be executed in the current shell. Anything written to stdout can be captured as a variable. The shell command will be evaluated as a CGPipe string and any variables substituted. . Example: . submit_host = $(hostname) submit_date = $(date) . Shell escaping can also be used within strings, such as: . print &quot;The current time is: $(date)&quot; . Printing . You can output arbitrary messages using the &quot;print&quot; statement. The default output is stdout, but this can be silenced using the -s command-line argument. . Example: . print &quot;Hello world&quot; foo = &quot;bar&quot; print &quot;foo${bar}&quot; . If/Else/Endif . Basic syntax: . if [condition] do something... elif [condition] do something... else do something else... endif . Conditions . if foo - if the variable ${foo} was set if !foo - if the variable ${foo} was not set or is false . if foo == &quot;bar&quot; - if the variable foo equals the string &quot;bar&quot; if foo != &quot;bar&quot; - if the variable foo doesn&#39;t equal the string &quot;bar&quot; . if foo &lt; 1 if foo &lt;= 1 if foo &gt; 1 if foo &gt;= 1 . For loops . Basic syntax: . for i in {start}..{end} do something... done for i in 1..10 do something... done for i in list do something... done . Build target definitions . Targets are the files that you want to create. They are defined on a single line listing the outputs of the target, a colon (:), and any inputs that are needed to build the outputs. . Any text (indented) after the target definition will be included in the script used to build the outputs. The indentation for the first line will be removed from all subsequent lines, in case there is a need for indentation to be maintained. The indentation can be any number of tabs or spaces. The first (non-blank) line that is at the same indentation level as the target definition line marks the end of the target definition. . CGPipe expressions can also be evaluated within the target definition. These will only be evaluated if the target needs to be built and can be used to dynamically alter the build script. Any variables that are defined within the target can only be used within the target. Any global variables are captured at the point when the target is defined. Global variables may not altered within a target, but they can be reset within the context of the target itself. . Example: . output1.txt.gz output2.txt.gz : input1.txt input2.txt gzip -c input1.txt &gt; output1.txt.gz gzip -c input2.txt &gt; output2.txt.gz . You may also have more than one target definition for any given output file(s). In the event that there is more than one way to build an ouput, the first listed build definition will be tried first. If the needed inputs (or dependencies) aren&#39;t available for the first definition, then the next will be tried until all methods are exhausted. . In the event that a complete build tree can&#39;t be found, a ParseError will be thrown. . Wildcards in targets . Using wildcards, the above could also be rewritten like this: . %.gz: % gzip -c $&lt; &gt; $&gt; . Note: The &#39;%&#39; is only valid as a wildcard placeholder for inputs / outputs. To use the wildcard in the body of the target, use $%. . Target substitutions . In addition to global variable substitutions, within a target these additional substitutions are available. Targets may also have their own local variables. . Note: For global variables, their values are captured when a target is defined. . $&gt; - The list of all outputs $&gt;num - The {num}&#39;th output (starts at 1) $&lt; - The list of all inputs $&lt;num - The {num}&#39;th input (starts at 1) $% - The wildcard match . Special targets . There are five special target names that can be added for any pipeline: __pre__, __post__, __setup__, __teardown__, and __postsubmit__. These are target definitions that accept no input dependencies. __pre__ is automatically added to the start of the body for all targets. __post__ is automatically added to the end of the body for all targets. __setup__ and __teardown__ will always run as the first and last job in the pipeline. __postsubmit__ is a new job that is run after each other job has been submitted. There will be only one __teardown__ job for the entire pipeline, but a separate __postsubmit__ job for each other job submitted. __postsubmit__ is always a shexec block and can be used to add monitoring based on the newly submitted job-id. For example, if you&#39;d like to keep track of jobs that were submitted, this could be used to add the new job&#39;s info (and job-id) to a database. . You can selectively disable __pre__ and __post__ for any job by setting the variable job.nopre and job.nopost. . Including other files . Other Pipeline files can be imported into the currently running Pipeline by using the include filename statement. In this case, the directory of the current Pipeline file will be searched for &#39;filename&#39;. If it isn&#39;t found, then the current working directory will be searched. If it still isn&#39;t found, then an ParseError will be thrown. . Logging . You can define a log file to use within the Pipeline file. You can do this with the log filename directive. If an existing log file is active, then it will be closed and the new log file used. By default all output from the Pipeline will be written to the last log file specified. . You may also specify a log file from the command-line with the -l logfile command-line argument. . Output logs . You can keep track of which files are scheduled to be created using an output log. Do use this, you&#39;ll need to set the cgpipe.joblog variable. If you set a joblog, then in addition to checking the local filesystem to see if a target already exists, the joblog will also be consulted. This file keeps track of outputs that have already been submitted to the job scheduler. CGPipe will also check with the job runner, to verify that the job is still valid (running or queued). . This way you can avoid re-submitting the same jobs over and over again if you re-run the pipeline. This output log enables the ability to have multiple pipelines coordinate common dependencies or chaining without requiring an external management daemon. This also allows you to write smaller separate (composable) pipelines instead of large monolithic ones. . Comments . Comments are started with a # character. You may also include the &#39;$&#39; and &#39;@&#39; characters in strings or evaluated lines by escaping them with a &#39;&#39; character before them, such as $. If they will be evaluated twice, you will need to escape them twice (as is the case with shell evaluated strings). . Help text . The user can request to disply help/usage text for any given pipeline. Any comment lines at the start of the file will be used as the help/usage text. The first non-comment line (including blank lines) will terminate the help text. If the script starts with a shebang (#!), then that line will not be included in the help text. . Example: . #!/usr/bin/env cgpipe # # This is a pipeline # # Options: # --gzip compress output # --input filename input filename # # (end of the help text) ... # rest of the script . Job execution options . Specifying the shell to use . CGPipe will attempt to find the correct shell interpreter to use for executing scripts. By default it will look for /bin/bash, /usr/bin/bash, /usr/local/bin/bash, or /bin/sh (in order of preference). Alternatively, you may set the config value cgpipe.shell in the $HOME/.cgpiperc file to set a specific shell binary. . The shell may also be chosen on a per-job basis by setting the job.shell variable for each job. . Direct execution of jobs . Certain jobs can also be directly executed as part of the pipeline building process. Instead of submitting these jobs to a scheduler, the jobs can be put into a temporary shell script and executed directly. The global shell will be used to run the script. Only jobs without any dependencies can be executed in this manner. If you would like a job to just run directly without being scheduled, set the variable job.shexec=true. Also, the __setup__ and __teardown__ can be executed as shexec. . One use for this is to setup any output folders that may be required. For example: . __setup__: &lt;% job.shexec = true %&gt; mkdir -p output . Another common use-case for this is having a clean target to remove all output files to perform a fresh set of calculations. For example: . clean: &lt;% job.shexec = true %&gt; rm *.bam . Experimental cgpipe language features . The following features are experimental. Syntax for the below may change in future versions of CGPipe (or be removed entirely). . Target snippets imports . Sometimes you might have more than one target definition that has the same (or similar) job body. In this case, you might want to have only one copy of the source snippet and import that copy into each separate build-target script. . You can do this with an &quot;importable&quot; target definition. This is one way to include a common snippet into a target script that isn&#39;t __pre__ or __post__. Importable target definitions are targets that have only one output (the name), followed by two colons. That snippet can then be imported into the body of a target definition using the import statement. . (Note: the import statement only works within the context of a build-target. If you need something like import in a Pipeline, try the include statement.) . Here&#39;s an example: . common:: echo &quot;this is the common snippet&quot; out.txt: input.txt &lt;% import common %&gt; out2.txt: input2.txt &lt;% import common %&gt; . Eval statement . The eval statment lets us eval a string at runtime . i=1 a=&quot;i=i+1&quot; eval a i =&gt; 2 . Double evaluated variables . Double var eval is useful in targets to include chunks of text based on a var: . foo=&quot;echo &quot;$&gt; &quot;&quot; target: $ . will result in this being added to the body: . echo &quot;target&quot; .",
    "url": "http://0.0.0.0:4000/cgpipe/docs/syntax",
    "relUrl": "/docs/syntax"
  }
  ,"6": {
    "title": "Job scheduling",
    "content": "Pipeline backends (batch schedulers) . . HPC server backends | Specifying requirements | Runner specific configuration Template scripts | Shell script export | Simple Batch Scheduler (SBS) | PBS (Torque/PBS) | SGE/OGE | SLURM | | Dry-runs | Job pipeline graph | Unlike Make, cgpipe does not actually execute pipeline commands. Instead, running the commands is outsourced to a &quot;job runner&quot; or backend. There are a number of supported backends, covering some of the more common HPC schedulers. . There are curently 5 available backends for running pipelines: a combined shell script (default), SGE/Open Grid Engine, PBS, SLURM, and single-user SBS (also from compgen.io, see below). . Job runners are chosen by setting the configuration value cgpipe.runner in $HOME/.cgpiperc to either: &#39;pbs&#39;, &#39;sge&#39;, &#39;slurm&#39;, &#39;sbs&#39;, or &#39;shell&#39; (default). . Example: . cgpipe.runner=&quot;sbs&quot; cgpipe.runner.sbs.sbshome=&quot;/home/username/.sbs&quot; . Like other cgpipe-wide configuration variables, any of these values can be changed on a per-run, per-user, or per-server basis. . HPC server backends . The most common use-case for CGPipe is running jobs within an HPC context. Currently, the only HPC job schedulers that are supported are SGE/Open Grid Engine and SLURM. CGPipe integrates with these schedulers by dynamically generating job scripts and submitting them to the scheduler by running scheduler-specific programs (qsub/sbatch). . Specifying requirements . Resource requirements for each job (output-target) can be set on a per-job basis by setting CGPipe variables. Because of the way that variable scoping works, you can set any of the variables below at the script or job level. . job setting description shell sge slurm pbs sbs . job.name | Name of the job |   | X | X | X | X | . job.procs | Number of CPUs (per node) |   | X | X | X | X | . job.walltime | Max wall time for the job |   | X | X | X |   | . job.mem | Req&#39;d RAM (ex: 2M, 4G) [*] |   | X | X | X | X | . job.stack | Req&#39;d stack space (ex: 10M) |   | X |   |   |   | . job.hold | Place a user-hold on the job |   | X | X | X | X | . job.env (T/F) | Capture the current ENV vars |   | X | X | X |   | . job.qos | QoS setting |   |   | X | X |   | . job.project | Project setting |   | X |   |   |   | . job.priority | Priority setting |   | X |   |   |   | . job.nice | Job &quot;nice&quot; setting |   |   |   | X |   | . job.queue | Specific queue to submit job to |   |   |   | X |   | . job.wd | Working directory |   | X | X | X | X | . job.account | Billing account |   | X | X | X |   | . job.mail | Mail job status |   | X | X | X | X | . job.mailtype | When to send mail |   | [1] | [2] | X |   | . job.src | Write the submitted script to a file |   | X | X | X | X | . job.stdout | Capture stdout to file |   | X | X | X | X | . job.stderr | Capture stderr to file |   | X | X | X | X | . job.shell | Job-specific shell binary | [3] | X | X | X |   | . job.node.property | Property requirement for an exec node |   |   |   | X |   | . job.node.hostname | Exact host to run job on |   |   |   | X |   | . global job setting | description |   |   |   |   |   | . job.shexec(T/F) | Exec job; don&#39;t submit job | X | X | X | X | X | . job.nopre (T/F) | Don&#39;t include global pre | [4] | X | X | X | X | . job.nopost(T/F) | Don&#39;t include global post | [4] | X | X | X | X | . [*] Memory should be specified as the total amount required for the job, if necessary, cgpipe will re-calculate the per-processor required memory. . [1], [2] - job.mailtype has slightly different meanings for SGE and SLURM. The possible values are different for each scheduler. . [3] - the shell for the shell runner can be set using the global shell config, but the defaults are &quot;/bin/bash&quot;, &quot;/usr/bin/bash&quot;, &quot;/usr/local/bin/bash&quot;, &quot;/bin/sh&quot; (in that order of priority). . [4] - pre and post script are only included once for the shell runner, so if any job includes a pre or post, then the final script will as well. . Runner specific configuration . Runner configurations may be set using the form: cgpipe.runner.{runner_name}.{option}. . Each runner has different options, which are listed below. . For PBS, SGE, SLURM, and SBS, you have the option: cgpipe.runner.{runner_name}.global_hold. If global_hold is true, then each job will have a user-hold placed on it until the entire pipeline has been submitted. Once the entire pipeline has been submitted (successfully), the user-hold will be released and the pipeline can start. This is useful to make sure that any step of the pipeline will run if and only if the entire pipeline was able to be submitted. This also makes sure that quick running jobs don&#39;t finish before their child jobs have been submitted. If there is an issue with submitting any of the jobs, then all jobs will be aborted before they are released for execution. . For PBS, SGE, and SLURM, you can also set a global default account by using the cgpipe.runner.{runner_name}.account option. . Template scripts . PBS, SGE, SLURM, and SBS job runners all operate by processing a job template, setting the appropriate variables, and then calling the appropriate executable to submit the job (qsub, sbatch, or sbs). A basic job template is included in CGPipe for each of these schedulers; however, if you&#39;d like to use your own template, this can be specified by setting the variable cgpipe.runner.{runner_name}.template. As with all other options, this can be done on an adhoc, per-user or per-host basis. If you&#39;d like to write your own templates, the templates are themselves written as CGPipe scripts and can include logic and flow-control. . PBSTemplateRunner.template.cgp | SBSTemplateRunner.template.cgp | SGETemplateRunner.template.cgp | SLURMTemplateRunner.template.cgp | . Shell script export . Shell scripts will write a single script that contains all of the tasks for a given pipeline as functions in the script. When the script is executed, each of the functions will be executed in order. In case multiple pipelines need to be run, the variable cgpipe.runner.shell.filename can be set. If this is set, then each successive pipeline will be added to this single script file. CGPipe will manage the appropriate job names to avoid collisions. Additionally, if you write the output to a file, the presence of each output will be checked by the script. This means that only if an output file is missing will a job be executed. . By default the script is written to stdout. . The shell runner has one specific option that can be set: cgpipe.runner.shell.autoexec. If this is set, then instead of writing the assembled shell script to stdout, the script will be immediately executed. . Simple Batch Scheduler (SBS) . https://github.com/compgen-io/sbs . SBS has two other options (cgpipe.runner.sbs.): sbshome and path. sbshome sets where the SBS job scripts will be tracked. By default this is in the current directory under .sbs. However, this can be set by the $SBSHOME environmental variable or overridden by this property. path is the path to the sbs program, if it isn&#39;t part of your $PATH. . PBS (Torque/PBS) . PBS has three unique options (cgpipe.runner.pbs.): trim_jobid, use_vmem and ignore_mem. trim_jobid will trim away the cluster name from the jobid returned from qsub. If your cluster returns something like &quot;1234.cluster.hostname.org&quot; from qsub, but requires &quot;1234&quot; for qstat, etc… then this option will trim away the &quot;cluster.hostname.org&quot; from the captured jobid. use_vmem will set all memory restrictions with -l vmem=XX as opposed to the default -l mem=XX. And ignore_mem will ignore any memory restrictions whatsoever in the job submission script (this may cause problems with your cluster or job scheduler, so use at your own risk). . SGE/OGE . For SGE, there are two additional options (cgpipe.runner.sge.): the name of the parallel environment needed to request more than one slot per node (parallelenv; -pe in qsub), and if the memory required should be specified per job or per slot (hvmem_total; -l h_vmem in qsub). The default parallelenv is named &#39;smp&#39; and by default h_vmem is specified on a per-slot basis (hvmem_total=F). . SLURM . SLURM has no additional options that haven&#39;t already been mentioned above. . Dry-runs . If you want to see the script that would be sent to the job scheduler, you can specify both the verbose (-v) and dry-run (-dr) options to cgpipe and the job scripts that would be submitted will be printed to stdout as opposed to being sent to the job scheduler. This is a good way to troubleshoot parameters and custom templates if needed. . These options can be sent to any cgpipe command, or if a script uses the #!cgpipe or #!/usr/bin/env cgpipe shebang syntax, these options can be provided directly to that script. . Options with a single dash (-) are sent to cgpipe itself whereas options with a double dash (--) are used as argument to your pipeline. . The shell job-runner is also very useful for troubleshooting pipelines, so that one can verify if the job recipe scripts are correct. . Job pipeline graph . An included non-executing job runner is also available that converts the job dependency graph into a graphviz model. To use this runner, you need to set cgpipe.runner=&quot;graphviz&quot;. Instead of executing the job tree, this will produce a graphviz dot file to show the dependencies between jobs. This can be used to generate a PDF or PNG image using the Graphviz tools. This does not currently support displaying multiple interconnected pipelines through the the joblog, but that is a possibility for a future version. .",
    "url": "http://0.0.0.0:4000/cgpipe/docs/running-jobs",
    "relUrl": "/docs/running-jobs"
  }
  ,"7": {
    "title": "Tutorials",
    "content": "Tutorials . The easiest way to get started with writing CGPipe pipelines is to see a few examples. . We&#39;ll start with a few easy examples and then work up to some fully fleshed out examples (that are in production use). .",
    "url": "http://0.0.0.0:4000/cgpipe/docs/tutorials",
    "relUrl": "/docs/tutorials"
  }
  ,"8": {
    "title": "",
    "content": "404 . Page not found :( . The requested page could not be found. Maybe try the search bar above? .",
    "url": "http://0.0.0.0:4000/cgpipe/404.html",
    "relUrl": "/404.html"
  }
  ,"9": {
    "title": "Remote pipelines",
    "content": "Remote pipelines . Note: this is an experimental feature . Pipeline files don&#39;t have to be on the local filesystem in order to be run. Remote pipelines can be used anywhere that a local pathname could be used for CGPipe. Remote pipelines can either be given as explicit HTTP or HTTPS URLs or use a &quot;named remote&quot;. Named remotes are web addresses that can be accessed using a shortcut naming scheme to make it easier to use interactively. . Defining custom named remote sources . Defining a new named remote source can be accomplished by setting a new variable in CGPipe (either from a pipeline or from a global RC script). The new value should be cgpipe.remote.$shortname$.baseurl. The value should be the base-url to use for the resource. As an example: . cgpipe.remote.compgen_io.baseurl = &quot;https://raw.githubusercontent.com/compgen-io/cgpipe-pipelines/master/&quot; . You can then load a remote pipeline using the remote-name:filename syntax. As an example, to see the help text for the pipeline compgen_io:pipelines, you&#39;d be able to run the following: . cgpipe -h -f compgen_io:pipelines . where the pipeline script itself is loaded from https://raw.githubusercontent.com/compgen-io/cgpipe-pipelines/master/pipelines. . SHA-1 hashes . Because remote pipelines can be updated at any time, it is important to be able to track if a pipeline is what you expect it to be. This can either mean logging the SHA-1 hash of the script, actively verifying the SHA-1 hash of a local or remote pipeline. The current filename and hash is available with the special variables cgpipe.current.filename and cgpipe.current.hash. . Note: If tracking the versions of pipelines is critical (as in version-controlled pipelines), then it is recommended that any remote pipelines be located on a server controlled by you and that each file hash is verified as the pipeline is loaded. . You can verify any local or remote pipelines by including the expected SHA-1 hash in the filename as shown below. . From within the pipeline: . `include remote/filename#expected-sha1-hash` . or using the cgpipe executable syntax: . `cgpipe -f &quot;localfile#sha1-hash&quot;` (Note the quotes - otherwise the `#` would be interpreted as a comment) .",
    "url": "http://0.0.0.0:4000/cgpipe/docs/features/remote-pipelines",
    "relUrl": "/docs/features/remote-pipelines"
  }
  ,"10": {
    "title": "About",
    "content": "History of cgpipe . CGpipe was written by Marcus Breese, PhD to support managing computational workflows in the study of pediatric cancers at Stanford University and the University of California San Francisco. For each patient sample sequenced, hundreds of jobs are generated and processed. This required the use of many different high-performance computing clusters over the years, including PBS, SGE, and SLURM clusters. The difficulty in migrating pipelines and adapting them to the different systems was one of the main motivations for cgpipe. . However, the major motivation was the desire to the the writing and use of complex pipelines as comparable to writing a simple shell script. For example, processing a single RNAseq sample may require 10&#39;s of jobs, but most of the logic is consistent between runs. There are a few variables (which FASTQ files to use, which reference genome, etc…), but for the most part, these can be handled with simple command line arguments. Then question then became – how can we make something that works like this: . $ ./rnaseq --fq1 input_R1.fastq --fq2 input_R2.fastq --ref GRCh38.fa --output sample1/ . This is type of dataflow that cgpipe was written to support. With this as the model, cgpipe has refined over the years to process hundreds of thousands of jobs. If this is something that you need for data pipeline work, give it a try. . If you have any questions or find any issues with cgpipe, please contact me! (Or submit an issue on github!) .",
    "url": "http://0.0.0.0:4000/cgpipe/about",
    "relUrl": "/about"
  }
  ,"11": {
    "title": "Back to compgen.io",
    "content": "",
    "url": "http://0.0.0.0:4000/cgpipe/back_to_compgen.html",
    "relUrl": "/back_to_compgen.html"
  }
  ,"12": {
    "title": "Help!",
    "content": "Help! . For help or questions about cgpipe, please contact: . Marcus R. Breese, PhD marcus.breese@ucsf.edu . Github links . Code repository | Github issues | .",
    "url": "http://0.0.0.0:4000/cgpipe/contact",
    "relUrl": "/contact"
  }
  ,"13": {
    "title": "Download",
    "content": "",
    "url": "http://0.0.0.0:4000/cgpipe/download.html",
    "relUrl": "/download.html"
  }
  ,"14": {
    "title": "Features",
    "content": "Features . Here are some of the features that makes cgpipe unique and helps to scale analysis pipelines. .",
    "url": "http://0.0.0.0:4000/cgpipe/docs/features",
    "relUrl": "/docs/features"
  }
  ,"15": {
    "title": "Home",
    "content": "cgpipe – a language for bioinformatics pipelines . CGpipe is a bioinformatics pipeline development language. It is very similar in spirit (and practice) to the venerable Makefile, but geared towards execution in an HPC environment. . Like Makefiles, cgpipe pipelines establish a series of build targets, their required dependencies, and a recipe (shell script) used to build the outputs. . Unlike Makefiles, cgpipe pipelines are scripts that can incorporate logic and flow control in the setup and execution of the build-graph. Additionally, cgpipe doesn&#39;t actually execute your job scripts – instead, it submits these jobs to a proper job scheduler for efficient distributed processing across a computational cluster. . CGpipe is distributed as a self-executing fat-JAR file. This means that for installation, all one needs is a working copy of Java and the cgpipe file. .",
    "url": "http://0.0.0.0:4000/cgpipe/",
    "relUrl": "/"
  }
  
}